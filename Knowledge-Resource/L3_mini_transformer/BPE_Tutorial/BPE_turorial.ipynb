{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mini Transformer - BPE Tokenization + Cosine LR Warmup/Decay (Hands-on Notebook)\n",
    "    - **Preparing a corpus** (Your own text/code/chats/docs)\n",
    "    - **Training a BPE Tokenizer** (Hugging Face tokenizers) - with a SentencePiece alternative.\n",
    "    - **Wiring the tokenizer into a Tiny GPT-style Transformer**\n",
    "    - **Training with cosine LR + warmup, AMP, and gradient accumulation.\n",
    "    - **Sample text**\n"
   ],
   "id": "3453f4302fb8f877"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "0) Setup:\n",
    "    - Install deps\n",
    "        - !pip -q install torch tqdm tokenizers sentencepiece"
   ],
   "id": "3f8aab3faa4e863e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:53:15.211723Z",
     "start_time": "2025-10-18T08:53:14.109114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0) Setup\n",
    "!pip -q install torch tqdm tokenizers\n",
    "import os, math, glob, json\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import trange\n",
    "print('Torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())"
   ],
   "id": "3bd2361f11231b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Torch: 2.9.0 | CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1) Bring/Build/Load your Corpus\n",
    "You can: (a) download tiny Shakespeare, (b) point to a folder of .txt, .md, .code files, or (c) paste text.\n",
    "### Tip:\n",
    "    - For code include extensions like .py, .js, .ts, .java, .go.\n",
    "    - For chats, export them to .txt and drop into a folder."
   ],
   "id": "55b08c66e4685617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:53:22.800683Z",
     "start_time": "2025-10-18T08:53:21.613041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Option A: tiny Shakespear(quick start)\n",
    "# import urllib.request\n",
    "# url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "# os.makedirs('data', exist_ok=True)\n",
    "# urllib.request.urlretrieve(url, 'data/input.txt')\n",
    "# print ('Saved data/input.txt')\n",
    "#\n",
    "# # Option B: Point to a folder of your own files (set This to your path)\n",
    "# USER_CORPUS_DIR = None      # e.g., '/path/to/my/corpus'\n",
    "#\n",
    "# def load_corpus(user_dir: str | None, min_len = 100):\n",
    "#     texts: List[str] = []\n",
    "#     if user_dir and os.path.isdir(user_dir):\n",
    "#         exts = ['*.txt', '*.md', '*.py', '*.js', '*.ts', '*.java', '*.go', '*.rs', '*.c', '*.cpp', '*.h']\n",
    "#         for ext in exts:\n",
    "#             for p in glob.glob(os.path.join(user_dir, '**', ext), recursive=True):\n",
    "#                 try:\n",
    "#                     s = open(p, 'r', encoding = 'utf-8', errors='ignore').read()\n",
    "#                     if len(s) >= min_len:\n",
    "#                         texts.append(s)\n",
    "#                 except Exception as e:\n",
    "#                     print('skip', p, e)\n",
    "#     else:\n",
    "#         # fallback to tiny shakespear\n",
    "#         texts.append(open('data/input.txt', 'r', encoding = 'utf-8').read())\n",
    "#     return texts\n",
    "#\n",
    "# texts = load_corpus(USER_CORPUS_DIR)\n",
    "# print('Loaded files: ', len(texts), 'Total chars:', sum(len(t) for t in texts))\n",
    "\n",
    "DATA_FILE = None  # e.g., 'data/input.txt'\n",
    "DATA_DIR  = None  # e.g., '/path/to/my/corpus'\n",
    "MIN_LEN_PER_FILE = 100  # skip very tiny files\n",
    "\n",
    "def load_texts(data_dir: Optional[str], data_file: Optional[str], min_len: int = 100) -> List[str]:\n",
    "    texts: List[str] = []\n",
    "    if data_file:\n",
    "        s = open(data_file, 'r', encoding='utf-8', errors='ignore').read()\n",
    "        if len(s) >= min_len:\n",
    "            texts.append(s)\n",
    "    elif data_dir:\n",
    "        exts = ['*.txt','*.md','*.py','*.js','*.ts','*.java','*.go','*.rs','*.c','*.cpp']\n",
    "        for ext in exts:\n",
    "            for p in glob.glob(os.path.join(data_dir, '**', ext), recursive=True):\n",
    "                try:\n",
    "                    s = open(p, 'r', encoding='utf-8', errors='ignore').read()\n",
    "                    if len(s) >= min_len:\n",
    "                        texts.append(s)\n",
    "                except Exception as e:\n",
    "                    print('skip', p, e)\n",
    "    else:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url, 'data/input.txt')\n",
    "        texts.append(open('data/input.txt','r',encoding='utf-8').read())\n",
    "    if not texts:\n",
    "        raise RuntimeError('No usable text found. Set DATA_FILE or DATA_DIR.')\n",
    "    print('Loaded files:', len(texts), '| Total chars:', sum(len(t) for t in texts))\n",
    "    return texts\n",
    "\n",
    "texts = load_texts(DATA_DIR, DATA_FILE, MIN_LEN_PER_FILE)\n",
    "\n"
   ],
   "id": "6255beeb8cd53964",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files: 1 | Total chars: 1115394\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2) Train a BPE Tokenizer (Hugging Face **Tokenizers**)\n",
    "    - we build a BPE vocabulary with special tokens. For small experiments, **vocab_size=2000-8000** is good.\n",
    "    - Alternative: SentencePiece is below if you prefer that library\n",
    "    - If bpe/tokenizer.json exists, we'll reuse it; otherwise we train a new one.\n",
    "        -  Guidance:\n",
    "                - Use `vocab_size=4k-8k` for small pure-text corpora\n",
    "                - Use `8k-16k` for mixed code + prose"
   ],
   "id": "99eee540fd10955f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:55:48.241472Z",
     "start_time": "2025-10-18T08:55:48.013902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "TOKENIZER_PATH = 'bpe/tokenizer.json'\n",
    "VOCAB_SIZE = 8000\n",
    "\n",
    "def maybe_train_tokenizer(texts: List[str], tok_path: str, vocab_size: int = 8000):\n",
    "    if os.path.exists(tok_path):\n",
    "        print(f'[tokenizer] using existing {tok_path}')\n",
    "        return\n",
    "    print(f'[tokenizer] training new BPE tokenizer → {tok_path} (vocab_size={vocab_size})')\n",
    "    os.makedirs(os.path.dirname(tok_path) or '.', exist_ok=True)\n",
    "    tok = Tokenizer(BPE(unk_token='<unk>'))\n",
    "    tok.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=['<pad>','<unk>','<bos>','<eos>'])\n",
    "    tok.train_from_iterator(texts, trainer)\n",
    "    tok.save(tok_path)\n",
    "\n",
    "maybe_train_tokenizer(texts, TOKENIZER_PATH, VOCAB_SIZE)\n",
    "tok = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print('Vocab size:', tok.get_vocab_size())\n",
    "print('Test decode:', tok.decode(tok.encode('Hello, mini Transformer!').ids))"
   ],
   "id": "d7135d9ef1601963",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenizer] training new BPE tokenizer → bpe/tokenizer.json (vocab_size=8000)\n",
      "\n",
      "\n",
      "\n",
      "Vocab size: 8000\n",
      "Test decode: He llo , min i Tr ans former !\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- (Optional) SentencePiece Alternative\n",
    "    - This does the same idea with the SentencePiece library."
   ],
   "id": "b9ab3d5f384f9b5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "corpus_path = 'bpe/corpus.txt'\n",
    "open(corpus_path, 'w', encoding='utf-8').write('\\n'.join(texts))\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=corpus_path,\n",
    "    model_prefix='bpe/spm',\n",
    "    vocab_size=4000,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    pad_id =0, unk_id=1, bos_id=2, eos_id=3,\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='bpe/spm.model')\n",
    "print('SPM Test:', sp.decode(sp.encode('Hello Transformer!', out_type=int)))\n"
   ],
   "id": "4e92775de1b0132c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3) Dataset using BPE\n",
    "    - we encode all texts to one long stream, add `<eos>` separators, split 90/10 train/val, and create batches by slicing fixed window\n",
    "    - This class will auto-clamp `block_size` if the train split is smaller than the requested context\n"
   ],
   "id": "6debfe905ad11e66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:57:59.818415Z",
     "start_time": "2025-10-18T08:57:59.587485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BPEDataset:\n",
    "    def __init__(self, texts: List[str], tokenizer_path: str, block_size: int = 256, device: Optional[str] = None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tok = Tokenizer.from_file(tokenizer_path)\n",
    "        ids = []\n",
    "        eos_id = self.tok.token_to_id('<eos>')\n",
    "        for t in texts:\n",
    "            enc = self.tok.encode(t).ids\n",
    "            if enc:\n",
    "                ids.extend(enc)\n",
    "                if eos_id is not None:\n",
    "                    ids.append(eos_id)\n",
    "        data = torch.tensor(ids, dtype=torch.long)\n",
    "        total_len = len(data)\n",
    "        if total_len < 4:\n",
    "            raise RuntimeError('Encoded corpus too small. Add more text or reduce block_size a lot.')\n",
    "        n = int(0.9 * total_len)\n",
    "        self.train_data, self.val_data = data[:n], data[n:]\n",
    "        if block_size >= len(self.train_data) - 1:\n",
    "            new_bs = max(2, len(self.train_data) - 2)\n",
    "            print(f'[BPEDataset] block_size {block_size} > train len; clamping to {new_bs}.')\n",
    "            block_size = new_bs\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = self.tok.get_vocab_size()\n",
    "        if len(self.val_data) <= self.block_size + 1:\n",
    "            print(f'[BPEDataset] Warning: tiny val split for block_size={self.block_size} (len={len(self.val_data)}).')\n",
    "\n",
    "    def get_batch(self, split: str, batch_size: int):\n",
    "        src = self.train_data if split == 'train' else self.val_data\n",
    "        hi = len(src) - self.block_size - 1\n",
    "        if hi <= 0:\n",
    "            raise RuntimeError(\n",
    "                f'Not enough {split} tokens for block_size={self.block_size}. len(src)={len(src)}. '\n",
    "                'Add more data or reduce block_size.'\n",
    "            )\n",
    "        idx = torch.randint(0, hi, (batch_size,))\n",
    "        x = torch.stack([src[i:i+self.block_size] for i in idx])\n",
    "        y = torch.stack([src[i+1:i+1+self.block_size] for i in idx])\n",
    "        return x.to(self.device), y.to(self.device)\n",
    "\n",
    "BLOCK_SIZE = 256  # lower to 64/128 if your corpus is tiny\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "bpe_ds = BPEDataset(texts, tokenizer_path=TOKENIZER_PATH, block_size=BLOCK_SIZE, device=DEVICE)\n",
    "print('vocab_size=', bpe_ds.vocab_size, '| block_size=', bpe_ds.block_size,\n",
    "      '| train_len=', len(bpe_ds.train_data), '| val_len=', len(bpe_ds.val_data))\n",
    "\n",
    "xb, yb = bpe_ds.get_batch('train', batch_size=4)\n",
    "print('batch shapes:', xb.shape, yb.shape)"
   ],
   "id": "222c43cda15f6a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size= 8000 | block_size= 256 | train_len= 248230 | val_len= 27582\n",
      "batch shapes: torch.Size([4, 256]) torch.Size([4, 256])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4) Mini GPT Model (Same as before)\n",
    "    - Decoder-only blocks with masked self-attention and LM head."
   ],
   "id": "cfa185b06ac66fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:58:29.950079Z",
     "start_time": "2025-10-18T08:58:29.938272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.qkv = nn.Linear(n_embed, 3 * n_embed, bias=False)\n",
    "        self.proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer('mask', mask.view(1,1,block_size,block_size))\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        nh = self.n_head\n",
    "        q = q.view(B,T,nh,-1).transpose(1,2)\n",
    "        k = k.view(B,T,nh,-1).transpose(1,2)\n",
    "        v = v.view(B,T,nh,-1).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
    "        att = self.mask[:,:,:T,:T].to(dtype=att.dtype, device=att.device) * att + \\\n",
    "              (1 - self.mask[:,:,:T,:T].to(dtype=att.dtype, device=att.device)) * float('-inf')\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = CausalSelfAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed), nn.GELU(), nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=384, n_head=6, n_layer=6, block_size=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None,:,:]\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "            if top_k and top_k > 0:\n",
    "                k = min(int(top_k), logits.size(-1))\n",
    "                v, _ = torch.topk(logits, k)\n",
    "                logits[logits < v[:, [-1]]] = -float('inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "        return idx"
   ],
   "id": "7b0a260bb34f0d60",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5) Training Loop - Cosine LR + Warmup, AMP, Grad Accum\n",
    "    - This mirros the improved train.py (L4_training_loop.py) now using BPE dataset"
   ],
   "id": "d1820d8b07ceb8df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T13:18:15.208024Z",
     "start_time": "2025-10-18T08:58:45.543783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniGPT(\n",
    "    vocab_size=bpe_ds.vocab_size,\n",
    "    n_embed=384, n_head=6, n_layer=6,\n",
    "    block_size=bpe_ds.block_size, dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "LR = 3e-4\n",
    "MAX_STEPS = 3000\n",
    "WARMUP_STEPS = 200\n",
    "USE_COSINE = True\n",
    "BATCH_SIZE = 128 if device.type == 'cuda' else 64\n",
    "GRAD_ACCUM_STEPS = 2 if device.type == 'cuda' else 1\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "def lr_factor(step: int) -> float:\n",
    "    if step < WARMUP_STEPS:\n",
    "        return max(1e-8, (step+1)/max(1, WARMUP_STEPS))\n",
    "    if not USE_COSINE:\n",
    "        return 1.0\n",
    "    progress = (step - WARMUP_STEPS) / max(1, MAX_STEPS - WARMUP_STEPS)\n",
    "    min_factor = 0.1\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi * progress))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss(iters=20) -> float:\n",
    "    model.eval()\n",
    "    s = 0.0\n",
    "    for _ in range(iters):\n",
    "        x, y = bpe_ds.get_batch('val', BATCH_SIZE)\n",
    "        _, loss = model(x, y)\n",
    "        s += float(loss.item())\n",
    "    model.train()\n",
    "    return s / max(1, iters)\n",
    "\n",
    "best = float('inf')\n",
    "for step in trange(MAX_STEPS, desc='training'):\n",
    "    fac = lr_factor(step)\n",
    "    for pg in opt.param_groups: pg['lr'] = LR * fac\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    micro_bs = max(1, BATCH_SIZE // max(1, GRAD_ACCUM_STEPS))\n",
    "    for _ in range(GRAD_ACCUM_STEPS):\n",
    "        x, y = bpe_ds.get_batch('train', micro_bs)\n",
    "        with autocast(enabled=(device.type == 'cuda')):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / max(1, GRAD_ACCUM_STEPS)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    scaler.unscale_(opt)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(opt); scaler.update()\n",
    "\n",
    "    if (step + 1) % 200 == 0 or (step + 1) == MAX_STEPS:\n",
    "        vl = eval_loss(20)\n",
    "        ppl = math.exp(vl) if vl < 20 else float('inf')\n",
    "        print(f\"\\nstep {step+1} | val_loss {vl:.4f} | ppl ~ {ppl:.2f}\")\n",
    "        if vl < best:\n",
    "            best = vl\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'config': {\n",
    "                    'vocab_size': bpe_ds.vocab_size,\n",
    "                    'n_embed': 384, 'n_head': 6, 'n_layer': 6,\n",
    "                    'block_size': bpe_ds.block_size, 'dropout': 0.1\n",
    "                },\n",
    "                'tokenizer_path': TOKENIZER_PATH\n",
    "            }, 'bpe_best.pt')\n",
    "            print('new best saved: bpe_best.pt')"
   ],
   "id": "e345de328eda573a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_9959/2382328104.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "training:   0%|          | 0/3000 [00:00<?, ?it/s]/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_9959/2382328104.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(device.type == 'cuda')):\n",
      "training:   7%|▋         | 200/3000 [12:38<7:06:33,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 200 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  13%|█▎        | 400/3000 [25:41<6:19:12,  8.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 400 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|██        | 600/3000 [44:50<5:46:31,  8.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 600 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  27%|██▋       | 800/3000 [1:30:55<5:17:14,  8.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 800 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  33%|███▎      | 1000/3000 [1:45:54<4:47:41,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 1000 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|████      | 1200/3000 [1:58:00<4:20:12,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 1200 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  47%|████▋     | 1400/3000 [2:10:06<3:51:53,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 1400 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  53%|█████▎    | 1600/3000 [2:22:12<3:21:45,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 1600 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|██████    | 1800/3000 [2:34:25<2:53:36,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 1800 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  67%|██████▋   | 2000/3000 [3:10:14<2:24:26,  8.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 2000 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  73%|███████▎  | 2200/3000 [3:22:58<1:58:35,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 2200 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|████████  | 2400/3000 [3:35:08<1:27:52,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 2400 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  87%|████████▋ | 2600/3000 [3:47:21<58:23,  8.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 2600 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  93%|█████████▎| 2800/3000 [4:00:35<29:59,  9.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 2800 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 3000/3000 [4:19:29<00:00,  5.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 3000 | val_loss nan | ppl ~ inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6) Sample with a prompt\n",
    "    - Use the same tokenizer and the best checkpoint to generate text."
   ],
   "id": "e3f91fb6c195e81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T13:25:01.931348Z",
     "start_time": "2025-10-18T13:25:01.879594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ck = torch.load('bpe_best.pt', map_location=device)\n",
    "conf = ck['config']\n",
    "model = MiniGPT(\n",
    "    vocab_size=conf['vocab_size'],\n",
    "    n_embed=conf['n_embed'], n_head=conf['n_head'], n_layer=conf['n_layer'],\n",
    "    block_size=conf['block_size'], dropout=conf['dropout']\n",
    ").to(device)\n",
    "model.load_state_dict(ck['model']); model.eval()\n",
    "tok = Tokenizer.from_file(ck.get('tokenizer_path', TOKENIZER_PATH))\n",
    "\n",
    "def generate_text(prompt: str, max_new_tokens=300, temperature=0.9, top_k=80):\n",
    "    ctx = torch.tensor([tok.encode(prompt).ids], dtype=torch.long, device=device)\n",
    "    out = model.generate(ctx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    return tok.decode(out[0].tolist())\n",
    "\n",
    "print(generate_text('ROMEO:', max_new_tokens=300, temperature=0.8, top_k=80))"
   ],
   "id": "97d75f299b9d2370",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bpe_best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m ck = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbpe_best.pt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m conf = ck[\u001B[33m'\u001B[39m\u001B[33mconfig\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      3\u001B[39m model = MiniGPT(\n\u001B[32m      4\u001B[39m     vocab_size=conf[\u001B[33m'\u001B[39m\u001B[33mvocab_size\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m      5\u001B[39m     n_embed=conf[\u001B[33m'\u001B[39m\u001B[33mn_embed\u001B[39m\u001B[33m'\u001B[39m], n_head=conf[\u001B[33m'\u001B[39m\u001B[33mn_head\u001B[39m\u001B[33m'\u001B[39m], n_layer=conf[\u001B[33m'\u001B[39m\u001B[33mn_layer\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m      6\u001B[39m     block_size=conf[\u001B[33m'\u001B[39m\u001B[33mblock_size\u001B[39m\u001B[33m'\u001B[39m], dropout=conf[\u001B[33m'\u001B[39m\u001B[33mdropout\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      7\u001B[39m ).to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/serialization.py:1484\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1481\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1482\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1484\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1485\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1486\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1487\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1488\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1489\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/serialization.py:759\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    757\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer: FileLike, mode: \u001B[38;5;28mstr\u001B[39m) -> _opener[IO[\u001B[38;5;28mbytes\u001B[39m]]:\n\u001B[32m    758\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m759\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    760\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    761\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/serialization.py:740\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    739\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike[\u001B[38;5;28mstr\u001B[39m]], mode: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m740\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'bpe_best.pt'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tips\n",
    "    - Increase VOCAB_SIZE to 8k-16k for mixed code + prose\n",
    "    - Increase BLOCK_SIZE (if you have enough data + memory) for longer context.\n",
    "    - Scale n_layer / n_embed if you have more compute\n",
    "    - Try different sampling settings: temperature 0.7-1.0, top_k 50-(vocab_size-1)"
   ],
   "id": "811fae44310be85d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
