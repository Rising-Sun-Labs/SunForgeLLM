{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ Topic for Training Speed & Memory\n",
    "    - AMP (mixed precision) - autocast + GradScaler, TF32/bfloat16 toggles\n",
    "    - `torch.compile`(inductor) - when/why/how, safe fallbacks\n",
    "    - Gradient checkpointing - huge memory saving, small speed tax\n",
    "    - Dataloader/batching tuning - keeping the GPU fed\n",
    "    - Quick profiling - tokens/sec, step-time, and `torch.profiler`.\n",
    "    - Memory checks - peak memory, OOM patterns, tips\n",
    "    NOTE: Supports: CPU, GPU"
   ],
   "id": "a187d4354d3dfff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "0) Setup & Imports",
   "id": "dc8d9b0e4454f740"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:18:59.752740Z",
     "start_time": "2025-10-18T17:18:59.264615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "from functools import reduce\n",
    "\n",
    "from sympy.geometry.entity import scale\n",
    "from sympy.physics.units import micro\n",
    "!pip -q install torch tqdm tokenizers\n",
    "import urllib\n",
    "import os, math, time, glob, urllib.request\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import trange\n",
    "print('PyTorch: ', torch.__version__, '| CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)\n"
   ],
   "id": "556dad7233eca9b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "PyTorch:  2.9.0 | CUDA available: False\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1) Model & Data - Minimal but Realistic\n",
    "    - This is a small decode-only Transformer and two data options:\n",
    "        - Synthetic dataset (fast to run, great for profiling throughput)\n",
    "        - Optional BPEDataset hook**: If you have bpe/tokenizer.json and text file from other can enable it below"
   ],
   "id": "4b597d53fc57c253"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:19:07.143046Z",
     "start_time": "2025-10-18T17:19:07.134069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.qkv = nn.Linear(n_embed, 3 * n_embed, bias=False)\n",
    "        self.proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer('mask', mask.view(1,1,block_size, block_size))\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        nh = self.n_head\n",
    "        q = q.view(B,T,nh,-1).transpose(1,2)\n",
    "        k = k.view(B,T,nh,-1).transpose(1,2)\n",
    "        v = v.view(B,T,nh,-1).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = CausalSelfAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed), nn.GELU(), nn.Linear(4 * n_embed, n_embed), nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=384, n_head=6, n_layer=6, block_size=256, dropout=0.1, grad_checkpointing = False):\n",
    "        super().__init__()\n",
    "        self.block_size= block_size\n",
    "        self.grad_checkpointing = grad_checkpointing\n",
    "        self.tok_emp = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emp = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size, bias = False)\n",
    "        self.apply(self.__init)\n",
    "    def __init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emp(idx)+self.pos_emp(pos)[None, :, :]\n",
    "        x = self.drop(x)\n",
    "        if self.grad_checkpointing and self.training:\n",
    "            import torch.utils.checkpoint as ckpt\n",
    "            for blk in self.blocks:\n",
    "                x = ckpt.checkpoint(blk, x, use_reentrant=False)\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss"
   ],
   "id": "235ad556647b5435",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2) Data - Synthetic or BPE stream\n",
    "    - If `bpe/tokenizer.json` exists but don't have local text files, we can auto-download Tiny Shakespeare when `AUTO_DOWNLOAD_TINY_SHAKESPEARE=True`.\n"
   ],
   "id": "cc5db9db690d9e60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:19:52.451174Z",
     "start_time": "2025-10-18T17:19:51.450358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USE_BPE = True\n",
    "TOKENIZER_PATH = 'bpe/tokenizer.json'\n",
    "DATA_DIR = None  # e.g., '/path/to/corpus'\n",
    "AUTO_DOWNLOAD_TINY_SHAKESPEARE = True\n",
    "\n",
    "VOCAB_SIZE = 8000\n",
    "BLOCK_SIZE = 256\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 32\n",
    "TRAIN_TOKENS = 200_000\n",
    "VAL_TOKENS   = 20_000\n",
    "\n",
    "def make_stream(n_tokens:int, vocab_size:int):\n",
    "    return torch.randint(0, vocab_size, (n_tokens,), dtype=torch.long)\n",
    "\n",
    "train_stream = make_stream(TRAIN_TOKENS, VOCAB_SIZE)\n",
    "val_stream   = make_stream(VAL_TOKENS,   VOCAB_SIZE)\n",
    "\n",
    "def get_batch(stream: torch.Tensor, block_size:int, batch_size:int, device='cpu'):\n",
    "    hi = len(stream) - block_size - 1\n",
    "    idx = torch.randint(0, hi, (batch_size,))\n",
    "    x = torch.stack([stream[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([stream[i+1:i+1+block_size] for i in idx])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "if USE_BPE and os.path.exists(TOKENIZER_PATH):\n",
    "    from tokenizers import Tokenizer\n",
    "    tok = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "    eos_id = tok.token_to_id('<eos>')\n",
    "    texts: List[str] = []\n",
    "    if DATA_DIR and os.path.isdir(DATA_DIR):\n",
    "        for ext in ['*.txt','*.md','*.py','*.js','*.ts','*.java','*.go','*.rs','*.c','*.cpp']:\n",
    "            for p in glob.glob(os.path.join(DATA_DIR, '**', ext), recursive=True):\n",
    "                try:\n",
    "                    s = open(p, 'r', encoding='utf-8', errors='ignore').read()\n",
    "                    if len(s) >= 100:\n",
    "                        texts.append(s)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not texts and AUTO_DOWNLOAD_TINY_SHAKESPEARE:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        urllib.request.urlretrieve(url, 'data/input.txt')\n",
    "        texts.append(open('data/input.txt','r',encoding='utf-8').read())\n",
    "        print('[BPE] Downloaded tiny Shakespeare as fallback')\n",
    "    if texts:\n",
    "        ids = []\n",
    "        for t in texts:\n",
    "            enc = tok.encode(t).ids\n",
    "            if enc:\n",
    "                ids.extend(enc)\n",
    "                if eos_id is not None:\n",
    "                    ids.append(eos_id)\n",
    "        data = torch.tensor(ids, dtype=torch.long)\n",
    "        n = int(0.9 * len(data))\n",
    "        train_stream, val_stream = data[:n], data[n:]\n",
    "        VOCAB_SIZE = tok.get_vocab_size()\n",
    "        print('[BPE] Using BPE stream | vocab_size=', VOCAB_SIZE, '| lengths:', len(train_stream), len(val_stream))\n",
    "    else:\n",
    "        print('[BPE] No texts found and AUTO_DOWNLOAD_TINY_SHAKESPEARE=False â†’ using synthetic stream')\n",
    "else:\n",
    "    print('BPE not enabled or tokenizer missing â†’ using synthetic stream')"
   ],
   "id": "1300e76d057fed45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BPE] Downloaded tiny Shakespeare as fallback\n",
      "[BPE] Using BPE stream | vocab_size= 8000 | lengths: 248230 27582\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Speed & Memory Toggles",
   "id": "45bbf5c17bac9ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:26:21.911961Z",
     "start_time": "2025-10-18T17:26:21.705015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_amp  = (device.type == 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_ckpt = True\n",
    "use_compile = True if hasattr(torch, 'compile') else False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('medium')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model = MiniGPT(VOCAB_SIZE, n_embed=384, n_head=6, n_layer=6, block_size=BLOCK_SIZE, dropout=0.1, grad_checkpointing=use_ckpt).to(device)\n",
    "if use_compile and device.type in ('cuda','cpu'):\n",
    "    try:\n",
    "        model = torch.compile(model, mode='max-autotune')\n",
    "        print('torch.compile: enabled')\n",
    "    except Exception as e:\n",
    "        print('torch.compile: disabled â†’', repr(e))\n",
    "        use_compile = False\n",
    "else:\n",
    "    print('torch.compile: not available on this backend')"
   ],
   "id": "aa52e72d0a881ad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile: enabled\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Training Loop - Warmup + cosine, AMP, grad accumulation",
   "id": "80f4ce843fdfd419"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:51:52.140335Z",
     "start_time": "2025-10-18T17:51:51.858761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LR = 3e-4\n",
    "MAX_STEPS = 400\n",
    "WARMUP_STEPS = 50\n",
    "USE_COSINE = True\n",
    "GRAD_ACCUM_STEPS = 2 if device.type == 'cuda' else 1\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "def lr_factor(step: int) -> float:\n",
    "    if step < WARMUP_STEPS:\n",
    "        return max(1e-8, (step+1)/max(1, WARMUP_STEPS))\n",
    "    if not USE_COSINE:\n",
    "        return 1.0\n",
    "    progress = (step - WARMUP_STEPS)/max(1, MAX_STEPS - WARMUP_STEPS)\n",
    "    min_factor = 0.1\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi*progress))\n",
    "\n",
    "def eval_loss(n_iter=10):\n",
    "    model.eval(); s=0.0\n",
    "    for _ in range(n_iter):\n",
    "        xb, yb = get_batch(val_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "        with torch.no_grad():\n",
    "            _, loss = model(xb, yb)\n",
    "        s += float(loss.item())\n",
    "    model.train(); return s/max(1,n_iter)\n",
    "\n",
    "for _ in range(5):\n",
    "    xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "    with autocast(enabled=use_amp, dtype=torch.bfloat16 if use_amp else None):\n",
    "        _, loss = model(xb, yb)\n",
    "    loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "t_start = time.perf_counter()\n",
    "tokens_processed = 0\n",
    "\n",
    "best = float('inf')\n",
    "for step in trange(MAX_STEPS):\n",
    "    fac = lr_factor(step)\n",
    "    for pg in opt.param_groups: pg['lr'] = LR*fac\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    micro_bs = max(1, BATCH_SIZE // max(1, GRAD_ACCUM_STEPS))\n",
    "    for _ in range(GRAD_ACCUM_STEPS):\n",
    "        xb, yb = get_batch(train_stream, BLOCK_SIZE, micro_bs, device)\n",
    "        with autocast(enabled=use_amp, dtype=torch.bfloat16 if use_amp else None):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / max(1, GRAD_ACCUM_STEPS)\n",
    "        scaler.scale(loss).backward()\n",
    "        tokens_processed += xb.numel()\n",
    "    scaler.unscale_(opt)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(opt); scaler.update()\n",
    "    if (step+1) % 100 == 0:\n",
    "        vl = eval_loss(5)\n",
    "        print(f\"val_loss: {vl:.4f}\")\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "elapsed = time.perf_counter() - t_start\n",
    "tok_per_sec = tokens_processed / elapsed\n",
    "print(f\"Elapsed: {elapsed:.2f}s | Tokens processed: {tokens_processed:,} | Tokens/sec: {int(tok_per_sec):,}\")"
   ],
   "id": "88ea93921a861744",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/453436596.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n",
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/453436596.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp, dtype=torch.bfloat16 if use_amp else None):\n",
      "/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "layer_norm(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     28\u001B[39m     xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m autocast(enabled=use_amp, dtype=torch.bfloat16 \u001B[38;5;28;01mif\u001B[39;00m use_amp \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m         _, loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m     loss.backward(); opt.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     33\u001B[39m torch.cuda.synchronize() \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:414\u001B[39m, in \u001B[36mOptimizedModule.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    404\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001B[32m    405\u001B[39m     warnings.warn(\n\u001B[32m    406\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing `torch.compile(module)` when there are global hooks on \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmodules (e.g., from `register_module_forward_hook`); this will\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    412\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    413\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m414\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:832\u001B[39m, in \u001B[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    829\u001B[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001B[32m    831\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m832\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    833\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    834\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config.verbose:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 68\u001B[39m, in \u001B[36mMiniGPT.forward\u001B[39m\u001B[34m(self, idx, targets)\u001B[39m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcheckpoint\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mckpt\u001B[39;00m\n\u001B[32m     67\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m         x = \u001B[43mckpt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_reentrant\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     70\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_compile.py:53\u001B[39m, in \u001B[36m_disable_dynamo.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     50\u001B[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     51\u001B[39m     fn.__dynamo_disable = disable_fn  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdisable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1044\u001B[39m, in \u001B[36mDisableContext.__call__.<locals>._fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1042\u001B[39m _maybe_set_eval_frame(_callback_from_stance(\u001B[38;5;28mself\u001B[39m.callback))\n\u001B[32m   1043\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1044\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1045\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1046\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:503\u001B[39m, in \u001B[36mcheckpoint\u001B[39m\u001B[34m(function, use_reentrant, context_fn, determinism_check, debug, early_stop, *args, **kwargs)\u001B[39m\n\u001B[32m    501\u001B[39m \u001B[38;5;66;03m# Runs pre-forward logic\u001B[39;00m\n\u001B[32m    502\u001B[39m \u001B[38;5;28mnext\u001B[39m(gen)\n\u001B[32m--> \u001B[39m\u001B[32m503\u001B[39m ret = \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    504\u001B[39m \u001B[38;5;66;03m# Runs post-forward logic\u001B[39;00m\n\u001B[32m    505\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m.attn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     40\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m.mlp(\u001B[38;5;28mself\u001B[39m.ln2(x))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/normalization.py:229\u001B[39m, in \u001B[36mLayerNorm.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    228\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m229\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43meps\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2901\u001B[39m, in \u001B[36mlayer_norm\u001B[39m\u001B[34m(input, normalized_shape, weight, bias, eps)\u001B[39m\n\u001B[32m   2891\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[32m   2892\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m   2893\u001B[39m         layer_norm,\n\u001B[32m   2894\u001B[39m         (\u001B[38;5;28minput\u001B[39m, weight, bias),\n\u001B[32m   (...)\u001B[39m\u001B[32m   2899\u001B[39m         eps=eps,\n\u001B[32m   2900\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m2901\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2902\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackends\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcudnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43menabled\u001B[49m\n\u001B[32m   2903\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: layer_norm(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
