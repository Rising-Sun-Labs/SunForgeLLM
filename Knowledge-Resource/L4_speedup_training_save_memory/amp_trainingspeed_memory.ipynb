{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ Topic for Training Speed & Memory\n",
    "    - AMP (mixed precision) - autocast + GradScaler, TF32/bfloat16 toggles\n",
    "    - `torch.compile`(inductor) - when/why/how, safe fallbacks\n",
    "    - Gradient checkpointing - huge memory saving, small speed tax\n",
    "    - Dataloader/batching tuning - keeping the GPU fed\n",
    "    - Quick profiling - tokens/sec, step-time, and `torch.profiler`.\n",
    "    - Memory checks - peak memory, OOM patterns, tips\n",
    "    NOTE: Supports: CPU, GPU"
   ],
   "id": "86191903423831b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "0) Setup & Imports",
   "id": "e4bfc4b5ecb324c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:24:15.613405Z",
     "start_time": "2025-10-18T18:24:15.094260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0) Setup & global safety guards\n",
    "!pip -q install torch tqdm tokenizers\n",
    "import os\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'  # ensure PT2 compile stays off\n",
    "try:\n",
    "    import torch\n",
    "    torch._dynamo.reset()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import math, time, glob, urllib.request\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import trange\n",
    "print('PyTorch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "c6c01b67dfe20b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "PyTorch: 2.9.0 | CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:24:27.996026Z",
     "start_time": "2025-10-18T18:24:27.992961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USE_BPE = False                  # set True to use BPE stream when tokenizer JSON is present\n",
    "TOKENIZER_PATH = 'bpe/tokenizer.json'\n",
    "DATA_DIR = None                  # e.g., '/path/to/text_or_code_corpus'\n",
    "AUTO_DOWNLOAD_TINY_SHAKESPEARE = True\n",
    "\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "USE_CKPT = True                  # gradient checkpointing via checkpoint_sequential\n",
    "\n",
    "BLOCK_SIZE = 256\n",
    "VOCAB_SIZE_SYN = 8000            # synthetic stream vocab size\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 32\n",
    "GRAD_ACCUM_STEPS = 2 if torch.cuda.is_available() else 1\n",
    "\n",
    "LR = 3e-4\n",
    "MAX_STEPS = 400\n",
    "WARMUP_STEPS = 50\n",
    "USE_COSINE = True\n",
    "CLIP_NORM = 1.0"
   ],
   "id": "8bfce24d4ec7ddb6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1) Model & Data - Minimal but Realistic\n",
    "    - This is a small decode-only Transformer and two data options:\n",
    "        - Synthetic dataset (fast to run, great for profiling throughput)\n",
    "        - Optional BPEDataset hook**: If you have bpe/tokenizer.json and text file from other can enable it below"
   ],
   "id": "47620cf79ddf485c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:24:43.731746Z",
     "start_time": "2025-10-18T18:24:43.722431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.qkv = nn.Linear(n_embed, 3 * n_embed, bias=False)\n",
    "        self.proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer('mask', mask.view(1,1,block_size,block_size))\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        nh = self.n_head\n",
    "        q = q.view(B,T,nh,-1).transpose(1,2)\n",
    "        k = k.view(B,T,nh,-1).transpose(1,2)\n",
    "        v = v.view(B,T,nh,-1).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = CausalSelfAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed), nn.GELU(), nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=384, n_head=6, n_layer=6, block_size=256,\n",
    "                 dropout=0.1, grad_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.grad_checkpointing = grad_checkpointing\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)[None,:,:]\n",
    "        x = self.drop(x)\n",
    "        if self.grad_checkpointing and self.training:\n",
    "            import torch.utils.checkpoint as ckpt\n",
    "            seq = nn.Sequential(*self.blocks)\n",
    "            x = ckpt.checkpoint_sequential(seq, segments=len(self.blocks), input=x)\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss"
   ],
   "id": "ef7dba0b330a4edd",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2) Data - Synthetic or BPE stream\n",
    "    - If `bpe/tokenizer.json` exists but don't have local text files, we can auto-download Tiny Shakespeare when `AUTO_DOWNLOAD_TINY_SHAKESPEARE=True`.\n"
   ],
   "id": "ab7bfa6e68c89689"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:25:00.648586Z",
     "start_time": "2025-10-18T18:25:00.639947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_stream(n_tokens:int, vocab_size:int):\n",
    "    return torch.randint(0, vocab_size, (n_tokens,), dtype=torch.long)\n",
    "\n",
    "def get_batch(stream: torch.Tensor, block_size:int, batch_size:int, device='cpu'):\n",
    "    hi = len(stream) - block_size - 1\n",
    "    if hi <= 0:\n",
    "        raise RuntimeError(f\"Stream too small for block_size {block_size}. len={len(stream)}\")\n",
    "    idx = torch.randint(0, hi, (batch_size,))\n",
    "    x = torch.stack([stream[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([stream[i+1:i+1+block_size] for i in idx])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def maybe_build_bpe_stream():\n",
    "    if not USE_BPE or not os.path.exists(TOKENIZER_PATH):\n",
    "        return None\n",
    "    from tokenizers import Tokenizer\n",
    "    tok = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "    eos_id = tok.token_to_id('<eos>')\n",
    "    texts: List[str] = []\n",
    "    if DATA_DIR and os.path.isdir(DATA_DIR):\n",
    "        for ext in ['*.txt','*.md','*.py','*.js','*.ts','*.java','*.go','*.rs','*.c','*.cpp']:\n",
    "            for p in glob.glob(os.path.join(DATA_DIR, '**', ext), recursive=True):\n",
    "                try:\n",
    "                    s = open(p, 'r', encoding='utf-8', errors='ignore').read()\n",
    "                    if len(s) >= 100:\n",
    "                        texts.append(s)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not texts and AUTO_DOWNLOAD_TINY_SHAKESPEARE:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        urllib.request.urlretrieve(url, 'data/input.txt')\n",
    "        texts.append(open('data/input.txt','r',encoding='utf-8').read())\n",
    "        print('[BPE] Downloaded tiny Shakespeare as fallback')\n",
    "    if not texts:\n",
    "        print('[BPE] No texts found; falling back to synthetic stream.')\n",
    "        return None\n",
    "    ids = []\n",
    "    for t in texts:\n",
    "        enc = tok.encode(t).ids\n",
    "        if enc:\n",
    "            ids.extend(enc)\n",
    "            if eos_id is not None:\n",
    "                ids.append(eos_id)\n",
    "    data = torch.tensor(ids, dtype=torch.long)\n",
    "    n = int(0.9 * len(data))\n",
    "    train_stream, val_stream = data[:n], data[n:]\n",
    "    vocab_size = tok.get_vocab_size()\n",
    "    print(f'[BPE] Using BPE stream | vocab_size={vocab_size} | lengths: {len(train_stream)}, {len(val_stream)}')\n",
    "    return train_stream, val_stream, vocab_size\n",
    "\n",
    "# Build streams\n",
    "bpe = maybe_build_bpe_stream()\n",
    "if bpe is None:\n",
    "    VOCAB_SIZE = VOCAB_SIZE_SYN\n",
    "    train_stream = make_stream(200_000, VOCAB_SIZE)\n",
    "    val_stream   = make_stream(20_000,  VOCAB_SIZE)\n",
    "else:\n",
    "    train_stream, val_stream, VOCAB_SIZE = bpe"
   ],
   "id": "b2fc06ca697df2dd",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Speed & Memory Toggles",
   "id": "4ec0dcb94d21f4a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:39:43.186832Z",
     "start_time": "2025-10-18T18:25:17.774137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('medium')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model = MiniGPT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_embed=384, n_head=6, n_layer=6,\n",
    "    block_size=BLOCK_SIZE, dropout=0.1,\n",
    "    grad_checkpointing=USE_CKPT\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "def lr_factor(step: int) -> float:\n",
    "    if step < WARMUP_STEPS:\n",
    "        return max(1e-8, (step+1)/max(1, WARMUP_STEPS))\n",
    "    if not USE_COSINE:\n",
    "        return 1.0\n",
    "    progress = (step - WARMUP_STEPS)/max(1, MAX_STEPS - WARMUP_STEPS)\n",
    "    min_factor = 0.1\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi*progress))\n",
    "\n",
    "def eval_loss(n_iter=10) -> float:\n",
    "    model.eval(); s=0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iter):\n",
    "            xb, yb = get_batch(val_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "            _, loss = model(xb, yb)\n",
    "            s += float(loss.item())\n",
    "    model.train(); return s/max(1, n_iter)\n",
    "\n",
    "# Warmup few steps (stabilize kernels)\n",
    "for _ in range(3):\n",
    "    xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "    dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "    with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "        _, loss = model(xb, yb)\n",
    "    loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "t_start = time.perf_counter()\n",
    "tokens_processed = 0\n",
    "\n",
    "for step in trange(MAX_STEPS):\n",
    "    fac = lr_factor(step)\n",
    "    for pg in opt.param_groups: pg['lr'] = LR * fac\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    micro_bs = max(1, BATCH_SIZE // max(1, GRAD_ACCUM_STEPS))\n",
    "    for _ in range(GRAD_ACCUM_STEPS):\n",
    "        xb, yb = get_batch(train_stream, BLOCK_SIZE, micro_bs, device)\n",
    "        dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "        with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / max(1, GRAD_ACCUM_STEPS)\n",
    "        scaler.scale(loss).backward()\n",
    "        tokens_processed += xb.numel()\n",
    "    scaler.unscale_(opt)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "    scaler.step(opt); scaler.update()\n",
    "    if (step+1) % 100 == 0 or (step+1) == MAX_STEPS:\n",
    "        vl = eval_loss(5)\n",
    "        print(f\"step {step+1}/{MAX_STEPS} | val_loss {vl:.4f}\")\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "elapsed = time.perf_counter() - t_start\n",
    "tok_per_sec = int(tokens_processed / max(elapsed, 1e-6))\n",
    "print(f\"Elapsed: {elapsed:.2f}s | Tokens processed: {tokens_processed:,} | ~{tok_per_sec:,} tok/s\")"
   ],
   "id": "f290474fbde97bd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/1364933662.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n",
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/1364933662.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
      "/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:560: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.9 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/1364933662.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 100/400 [03:30<13:46,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100/400 | val_loss 9.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 200/400 [07:04<09:19,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200/400 | val_loss 9.1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 300/400 [10:40<04:43,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300/400 | val_loss 9.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [14:18<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400/400 | val_loss 9.1950\n",
      "Elapsed: 858.85s | Tokens processed: 3,276,800 | ~3,815 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Training Loop - Warmup + cosine, AMP, grad accumulation",
   "id": "9bab1360f308fb52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:11:51.267330Z",
     "start_time": "2025-10-18T18:11:43.997009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "def lr_factor(step: int) -> float:\n",
    "    if step < WARMUP_STEPS:\n",
    "        return max(1e-8, (step+1)/max(1, WARMUP_STEPS))\n",
    "    if not USE_COSINE:\n",
    "        return 1.0\n",
    "    progress = (step - WARMUP_STEPS)/max(1, MAX_STEPS - WARMUP_STEPS)\n",
    "    min_factor = 0.1\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi*progress))\n",
    "\n",
    "def eval_loss(n_iter=10) -> float:\n",
    "    model.eval(); s=0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iter):\n",
    "            xb, yb = get_batch(val_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "            _, loss = model(xb, yb)\n",
    "            s += float(loss.item())\n",
    "    model.train(); return s/max(1, n_iter)\n",
    "\n",
    "# Warmup few steps\n",
    "for _ in range(3):\n",
    "    xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "    dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "    with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "        _, loss = model(xb, yb)\n",
    "    loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "t_start = time.perf_counter()\n",
    "tokens_processed = 0\n",
    "\n",
    "for step in trange(MAX_STEPS):\n",
    "    fac = lr_factor(step)\n",
    "    for pg in opt.param_groups: pg['lr'] = LR * fac\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    micro_bs = max(1, BATCH_SIZE // max(1, GRAD_ACCUM_STEPS))\n",
    "    for _ in range(GRAD_ACCUM_STEPS):\n",
    "        xb, yb = get_batch(train_stream, BLOCK_SIZE, micro_bs, device)\n",
    "        dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "        with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "            _, loss = model(xb, yb)\n",
    "            loss = loss / max(1, GRAD_ACCUM_STEPS)\n",
    "        scaler.scale(loss).backward()\n",
    "        tokens_processed += xb.numel()\n",
    "    scaler.unscale_(opt)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "    scaler.step(opt); scaler.update()\n",
    "    if (step+1) % 100 == 0 or (step+1) == MAX_STEPS:\n",
    "        vl = eval_loss(5)\n",
    "        print(f\"step {step+1}/{MAX_STEPS} | val_loss {vl:.4f}\")\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "elapsed = time.perf_counter() - t_start\n",
    "tok_per_sec = int(tokens_processed / max(elapsed, 1e-6))\n",
    "print(f\"Elapsed: {elapsed:.2f}s | Tokens processed: {tokens_processed:,} | ~{tok_per_sec:,} tok/s\")"
   ],
   "id": "1fd8259ffe762414",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/1528711846.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n",
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/1528711846.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
      "/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/Context.cpp:85.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Exception C++ compile error\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Command:\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Output:\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]  for benchmark choice DataProcessorChoiceCallerWrapper(<torch._inductor.codegen.cpp_template_kernel.CppTemplateCaller object at 0x115cc6e40>)\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Traceback (most recent call last):\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 571, in _run_compile_cmd\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     subprocess.run(\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]         cmd, cwd=cwd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     )\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py\", line 577, in run\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     raise CalledProcessError(retcode, process.args,\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]                              output=stdout, stderr=stderr)\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] subprocess.CalledProcessError: Command '['clang++', '/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.cpp', '-D', 'TORCH_INDUCTOR_CPP_WRAPPER', '-D', 'STANDALONE_TORCH_HEADER', '-D', 'C10_USING_CUSTOM_GENERATED_MACROS', '-D', 'CPU_CAPABILITY_NEON', '-D', 'AT_BUILD_ARM_VEC256_WITH_SLEEF', '-O3', '-DNDEBUG', '-fno-trapping-math', '-funsafe-math-optimizations', '-ffinite-math-only', '-fno-signed-zeros', '-fno-math-errno', '-fno-finite-math-only', '-fno-unsafe-math-optimizations', '-ffp-contract=off', '-shared', '-fPIC', '-undefined', 'dynamic_lookup', '-Wall', '-std=c++17', '-Wno-unused-variable', '-Wno-unknown-pragmas', '-Werror=ignored-optimization-argument', '-Xclang', '-fopenmp', '-I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13', '-I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include', '-I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include', '-o', '/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.so', '-lomp', '-lc10', '-L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib', '-L/Users/ankushraj/Desktop/Rising', 'Sun', 'Labs', 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib']' returned non-zero exit status 1.\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] The above exception was the direct cause of the following exception:\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Traceback (most recent call last):\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py\", line 59, in run\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     result = self.fn(*self.args, **self.kwargs)\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py\", line 2739, in precompile_with_captured_stdout\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     choice.precompile()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_template_kernel.py\", line 588, in precompile\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     self.bmreq.precompile()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/autotune_process.py\", line 837, in precompile\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     CppCodeCache.load(self.source_code, device_type=\"cpu\")\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2839, in load\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     return cls.load_async(*args, **kwargs)()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2822, in load_fn\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     result = worker_fn()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2851, in _worker_compile_cpp\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     builder.build()\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 2020, in build\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     run_compile_cmd(build_cmd, cwd=_build_tmp_dir)\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 593, in run_compile_cmd\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     _run_compile_cmd(cmd_line, cwd)\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 588, in _run_compile_cmd\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     raise exc.CppCompileError(cmd, output) from e\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] torch._inductor.exc.CppCompileError: C++ compile error\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Command:\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Output:\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.166000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Runtime error during autotuning: \n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] C++ compile error\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] \n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Command:\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/iu/ciuqmknzavcklpqyrj5lmnhbkvb5utmnjske4rvv6cvjxsz6kyww.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] \n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Output:\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] . \n",
      "E1018 23:41:46.479000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Ignoring this choice.\n",
      "Autotune Choices Stats:\n",
      "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"bmm\", \"best_time\": 4.658416999973269}\n",
      "AUTOTUNE bmm(192x256x64, 192x64x256)\n",
      "strides: [16384, 64, 1], [16384, 256, 1]\n",
      "dtypes: torch.float32, torch.float32\n",
      "  bmm 4.6584 ms 100.0% \n",
      "  cpp_CppMicroGemmFP32Vec_0 inf ms 0.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.3116 seconds and 0.2840 seconds precompiling for 2 choices\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Exception C++ compile error\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Command:\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Output:\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]  for benchmark choice DataProcessorChoiceCallerWrapper(<torch._inductor.codegen.cpp_template_kernel.CppTemplateCaller object at 0x115de9e50>)\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Traceback (most recent call last):\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 571, in _run_compile_cmd\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     subprocess.run(\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]         cmd, cwd=cwd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     )\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py\", line 577, in run\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     raise CalledProcessError(retcode, process.args,\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]                              output=stdout, stderr=stderr)\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] subprocess.CalledProcessError: Command '['clang++', '/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.cpp', '-D', 'TORCH_INDUCTOR_CPP_WRAPPER', '-D', 'STANDALONE_TORCH_HEADER', '-D', 'C10_USING_CUSTOM_GENERATED_MACROS', '-D', 'CPU_CAPABILITY_NEON', '-D', 'AT_BUILD_ARM_VEC256_WITH_SLEEF', '-O3', '-DNDEBUG', '-fno-trapping-math', '-funsafe-math-optimizations', '-ffinite-math-only', '-fno-signed-zeros', '-fno-math-errno', '-fno-finite-math-only', '-fno-unsafe-math-optimizations', '-ffp-contract=off', '-shared', '-fPIC', '-undefined', 'dynamic_lookup', '-Wall', '-std=c++17', '-Wno-unused-variable', '-Wno-unknown-pragmas', '-Werror=ignored-optimization-argument', '-Xclang', '-fopenmp', '-I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13', '-I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include', '-I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include', '-o', '/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.so', '-lomp', '-lc10', '-L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib', '-L/Users/ankushraj/Desktop/Rising', 'Sun', 'Labs', 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib']' returned non-zero exit status 1.\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] The above exception was the direct cause of the following exception:\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Traceback (most recent call last):\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py\", line 59, in run\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     result = self.fn(*self.args, **self.kwargs)\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py\", line 2739, in precompile_with_captured_stdout\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     choice.precompile()\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codegen/cpp_template_kernel.py\", line 588, in precompile\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     self.bmreq.precompile()\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/autotune_process.py\", line 837, in precompile\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     CppCodeCache.load(self.source_code, device_type=\"cpu\")\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2839, in load\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     return cls.load_async(*args, **kwargs)()\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2822, in load_fn\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     result = worker_fn()\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py\", line 2851, in _worker_compile_cpp\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     builder.build()\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 2020, in build\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     run_compile_cmd(build_cmd, cwd=_build_tmp_dir)\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 593, in run_compile_cmd\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     _run_compile_cmd(cmd_line, cwd)\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]   File \"/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py\", line 588, in _run_compile_cmd\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0]     raise exc.CppCompileError(cmd, output) from e\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] torch._inductor.exc.CppCompileError: C++ compile error\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Command:\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] Output:\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.570000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2820] [3/0] \n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Runtime error during autotuning: \n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] C++ compile error\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] \n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Command:\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/kc/ckcxv6e3mq4s7vcnqt2b4zufme7bksu33znnr3hxy3hq2sbf3lqo.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] \n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Output:\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Sun'\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Labs'\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] clang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] . \n",
      "E1018 23:41:46.879000 1990 .venv/lib/python3.13/site-packages/torch/_inductor/select_algorithm.py:2967] [3/0] Ignoring this choice.\n",
      "Autotune Choices Stats:\n",
      "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"bmm\", \"best_time\": 5.158916500022315}\n",
      "AUTOTUNE bmm(192x256x256, 192x256x64)\n",
      "strides: [65536, 256, 1], [16384, 64, 1]\n",
      "dtypes: torch.float32, torch.float32\n",
      "  bmm 5.1589 ms 100.0% \n",
      "  cpp_CppMicroGemmFP32Vec_1 inf ms 0.0% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.3099 seconds and 0.0255 seconds precompiling for 2 choices\n",
      "Autotune Choices Stats:\n",
      "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 10.86116700025741}\n",
      "AUTOTUNE addmm(8192x1536, 8192x384, 384x1536)\n",
      "strides: [0, 1], [384, 1], [1, 384]\n",
      "dtypes: torch.float32, torch.float32, torch.float32\n",
      "  addmm 10.8612 ms 100.0% \n",
      "  bias_addmm 10.8891 ms 99.7% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4244 seconds and 0.0001 seconds precompiling for 2 choices\n",
      "Autotune Choices Stats:\n",
      "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 7.783874999404361}\n",
      "AUTOTUNE addmm(8192x384, 8192x1536, 1536x384)\n",
      "strides: [0, 1], [1536, 1], [1, 1536]\n",
      "dtypes: torch.float32, torch.float32, torch.float32\n",
      "  addmm 7.7839 ms 100.0% \n",
      "  bias_addmm 7.8208 ms 99.5% \n",
      "SingleProcess AUTOTUNE benchmarking takes 0.3977 seconds and 0.0001 seconds precompiling for 2 choices\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 65, in forward\n",
      "    x = self.tok_emb(idx) + self.pos_emb(pos)[None,:,:]\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 65, in forward\n",
      "    x = self.tok_emb(idx) + self.pos_emb(pos)[None,:,:]\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 64, in forward\n",
      "    pos = torch.arange(0, T, device=idx.device)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 14, in forward\n",
      "    qkv = self.qkv(x)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 20, in forward\n",
      "    att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 24, in forward\n",
      "    y = att @ v\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 25, in forward\n",
      "    y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 39, in forward\n",
      "    x = x + self.attn(self.ln1(x))\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 26, in forward\n",
      "    y = self.resid_drop(self.proj(y))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 75, in forward\n",
      "    x = blk(x)\n",
      "  File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 40, in forward\n",
      "    x = x + self.mlp(self.ln2(x))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3100621812.py\", line 77, in forward\n",
      "    logits = self.head(x)\n",
      "\n"
     ]
    },
    {
     "ename": "InductorError",
     "evalue": "CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/q5/cq5plabjg5pvzrcrbtktyiynoyjujrplw7pbtmfiecohqrunp5zx.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -include /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/precompiled_headers/c7mv6jl773tdyux4rvslwqz7eud43eht3ks5rt5kvi2w5u5s4rr4.h -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/q5/cq5plabjg5pvzrcrbtktyiynoyjujrplw7pbtmfiecohqrunp5zx.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n\nOutput:\nclang++: error: no such file or directory: 'Sun'\nclang++: error: no such file or directory: 'Labs'\nclang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mInductorError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m     dtype = torch.bfloat16 \u001B[38;5;28;01mif\u001B[39;00m (USE_AMP \u001B[38;5;129;01mand\u001B[39;00m torch.cuda.is_available()) \u001B[38;5;28;01melse\u001B[39;00m torch.float32\n\u001B[32m     26\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m autocast(enabled=USE_AMP, dtype=dtype \u001B[38;5;28;01mif\u001B[39;00m USE_AMP \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m         _, loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     loss.backward(); opt.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     30\u001B[39m torch.cuda.synchronize() \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:414\u001B[39m, in \u001B[36mOptimizedModule.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    404\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001B[32m    405\u001B[39m     warnings.warn(\n\u001B[32m    406\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing `torch.compile(module)` when there are global hooks on \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmodules (e.g., from `register_module_forward_hook`); this will\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    412\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    413\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m414\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:845\u001B[39m, in \u001B[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    841\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m__cause__\u001B[39;00m  \u001B[38;5;66;03m# User compiler error\u001B[39;00m\n\u001B[32m    842\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ShortenTraceback \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    843\u001B[39m     \u001B[38;5;66;03m# Failures in the backend likely don't have useful\u001B[39;00m\n\u001B[32m    844\u001B[39m     \u001B[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m845\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.remove_dynamo_frames() \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001B[39;00m\n\u001B[32m    846\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    847\u001B[39m     \u001B[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001B[39;00m\n\u001B[32m    848\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:990\u001B[39m, in \u001B[36m_compile_fx_inner\u001B[39m\u001B[34m(gm, example_inputs, **graph_kwargs)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m990\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m InductorError(e, currentframe()).with_traceback(\n\u001B[32m    991\u001B[39m         e.__traceback__\n\u001B[32m    992\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    993\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    994\u001B[39m     TritonBundler.end_compile()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:974\u001B[39m, in \u001B[36m_compile_fx_inner\u001B[39m\u001B[34m(gm, example_inputs, **graph_kwargs)\u001B[39m\n\u001B[32m    972\u001B[39m TritonBundler.begin_compile()\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m974\u001B[39m     mb_compiled_graph = \u001B[43mfx_codegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_to_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgraph_kwargs\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    977\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m mb_compiled_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    978\u001B[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1695\u001B[39m, in \u001B[36mfx_codegen_and_compile\u001B[39m\u001B[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001B[39m\n\u001B[32m   1691\u001B[39m     fast_scheme = _InProcessFxCompile()\n\u001B[32m   1693\u001B[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001B[32m-> \u001B[39m\u001B[32m1695\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscheme\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcodegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_to_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1505\u001B[39m, in \u001B[36m_InProcessFxCompile.codegen_and_compile\u001B[39m\u001B[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001B[39m\n\u001B[32m   1487\u001B[39m         compiled_fn = AotCodeCompiler.compile(\n\u001B[32m   1488\u001B[39m             graph,\n\u001B[32m   1489\u001B[39m             wrapper_code.value,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1502\u001B[39m             ],\n\u001B[32m   1503\u001B[39m         )\n\u001B[32m   1504\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1505\u001B[39m     compiled_module = \u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1506\u001B[39m     compiled_fn = compiled_module.call\n\u001B[32m   1507\u001B[39m     compiled_fn_runner = \u001B[38;5;28mgetattr\u001B[39m(\n\u001B[32m   1508\u001B[39m         compiled_module, \u001B[33m\"\u001B[39m\u001B[33mrunner\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1509\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2319\u001B[39m, in \u001B[36mGraphLowering.compile_to_module\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2312\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompile_to_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> CompiledModule:\n\u001B[32m   2313\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[32m   2314\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mGraphLowering.compile_to_module\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2315\u001B[39m         phase_name=\u001B[33m\"\u001B[39m\u001B[33mcode_gen\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2316\u001B[39m         log_pt2_compile_event=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   2317\u001B[39m         dynamo_compile_column_us=\u001B[33m\"\u001B[39m\u001B[33minductor_code_gen_cumulative_compile_time_us\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2318\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m2319\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_compile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2329\u001B[39m, in \u001B[36mGraphLowering._compile_to_module\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2324\u001B[39m wrapper_code, _ = (\n\u001B[32m   2325\u001B[39m     \u001B[38;5;28mself\u001B[39m.codegen_with_cpp_wrapper() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cpp_wrapper \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.codegen()\n\u001B[32m   2326\u001B[39m )\n\u001B[32m   2328\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(wrapper_code, ValueWithLineMap):\n\u001B[32m-> \u001B[39m\u001B[32m2329\u001B[39m     mod = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_compile_to_module_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrapper_code\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(wrapper_code, FileBackedGraphModule):\n\u001B[32m   2331\u001B[39m     mod = wrapper_code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2397\u001B[39m, in \u001B[36mGraphLowering._compile_to_module_lines\u001B[39m\u001B[34m(self, wrapper_code)\u001B[39m\n\u001B[32m   2391\u001B[39m     trace_structured(\n\u001B[32m   2392\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minductor_output_code\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2393\u001B[39m         \u001B[38;5;28;01mlambda\u001B[39;00m: {\u001B[33m\"\u001B[39m\u001B[33mfilename\u001B[39m\u001B[33m\"\u001B[39m: path},\n\u001B[32m   2394\u001B[39m         payload_fn=\u001B[38;5;28;01mlambda\u001B[39;00m: wrapper_code.value,\n\u001B[32m   2395\u001B[39m     )\n\u001B[32m   2396\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\u001B[33m\"\u001B[39m\u001B[33mPyCodeCache.load_by_key_path\u001B[39m\u001B[33m\"\u001B[39m, log_pt2_compile_event=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m2397\u001B[39m     mod = \u001B[43mPyCodeCache\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_by_key_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2398\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2399\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2400\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlinemap\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlinemap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   2401\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconstants\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtorchbind_constants\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2402\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2403\u001B[39m \u001B[38;5;28mself\u001B[39m.cache_key = key\n\u001B[32m   2404\u001B[39m \u001B[38;5;28mself\u001B[39m.cache_path = path\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3548\u001B[39m, in \u001B[36mPyCodeCache.load_by_key_path\u001B[39m\u001B[34m(cls, key, path, linemap, attrs)\u001B[39m\n\u001B[32m   3545\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m.modules_no_attr[path]\n\u001B[32m   3547\u001B[39m in_toplevel = in_toplevel_process()\n\u001B[32m-> \u001B[39m\u001B[32m3548\u001B[39m mod = \u001B[43m_reload_python_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_sys_modules\u001B[49m\u001B[43m=\u001B[49m\u001B[43min_toplevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3550\u001B[39m \u001B[38;5;66;03m# unzip into separate lines/nodes lists\u001B[39;00m\n\u001B[32m   3551\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m in_toplevel:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/runtime/compile_tasks.py:33\u001B[39m, in \u001B[36m_reload_python_module\u001B[39m\u001B[34m(key, path, set_sys_modules)\u001B[39m\n\u001B[32m     31\u001B[39m mod.\u001B[34m__file__\u001B[39m = path\n\u001B[32m     32\u001B[39m mod.key = key  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m \u001B[43mexec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__dict__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__dict__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m set_sys_modules:\n\u001B[32m     35\u001B[39m     sys.modules[mod.\u001B[34m__name__\u001B[39m] = mod\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/vp/cvphkfundg27k3xghxaorxezcisvunxrgssuxtzhdnlvc3g2rmxu.py:3243\u001B[39m\n\u001B[32m   3018\u001B[39m cpp_fused_add_native_dropout_native_layer_norm_native_layer_norm_backward_view_36 = async_compile.cpp_pybinding([\u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst bool*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst float*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst float*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst float*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m], \u001B[33m'''\u001B[39m\n\u001B[32m   3019\u001B[39m \u001B[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001B[39m\n\u001B[32m   3020\u001B[39m \u001B[33mextern \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  void  kernel(float* in_out_ptr0,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3104\u001B[39m \u001B[33m}\u001B[39m\n\u001B[32m   3105\u001B[39m \u001B[33m'''\u001B[39m)\n\u001B[32m   3108\u001B[39m cpp_fused__log_softmax__unsafe_view_nll_loss_forward_view_37 = async_compile.cpp_pybinding([\u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst float*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mconst int64_t*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint64_t*\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfloat*\u001B[39m\u001B[33m'\u001B[39m], \u001B[33m'''\u001B[39m\n\u001B[32m   3109\u001B[39m \u001B[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001B[39m\n\u001B[32m   3110\u001B[39m \u001B[33mextern \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  void  kernel(float* in_out_ptr0,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3239\u001B[39m \u001B[33m}\u001B[39m\n\u001B[32m   3240\u001B[39m \u001B[33m'''\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m3243\u001B[39m \u001B[43masync_compile\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3244\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m async_compile\n\u001B[32m   3246\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mRunner\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:631\u001B[39m, in \u001B[36mAsyncCompile.wait\u001B[39m\u001B[34m(self, scope)\u001B[39m\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m get_compile_threads() > \u001B[32m1\u001B[39m:\n\u001B[32m    624\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\n\u001B[32m    625\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33masync_compile.wait\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    626\u001B[39m         log_pt2_compile_event=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    629\u001B[39m         waitcounter_name_override=\u001B[33m\"\u001B[39m\u001B[33mcompile_triton\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    630\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m631\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_wait_futures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    633\u001B[39m _compile_end()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:651\u001B[39m, in \u001B[36mAsyncCompile._wait_futures\u001B[39m\u001B[34m(self, scope)\u001B[39m\n\u001B[32m    649\u001B[39m     pbar.set_postfix_str(key)\n\u001B[32m    650\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m651\u001B[39m     kernel = \u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    652\u001B[39m     scope[key] = kernel\n\u001B[32m    653\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m BrokenProcessPool \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:4289\u001B[39m, in \u001B[36mLambdaFuture.result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   4288\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Callable[..., Any]:\n\u001B[32m-> \u001B[39m\u001B[32m4289\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mresult_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3025\u001B[39m, in \u001B[36mCppPythonBindingsCodeCache.load_pybinding_async.<locals>.future\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   3023\u001B[39m \u001B[38;5;28;01mnonlocal\u001B[39;00m result\n\u001B[32m   3024\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3025\u001B[39m     result = \u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3026\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, ModuleType)\n\u001B[32m   3027\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(result, \u001B[38;5;28mcls\u001B[39m.entry_function)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2821\u001B[39m, in \u001B[36mCppCodeCache.load_async.<locals>.load_fn\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   2819\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m lib \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2820\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m future \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2821\u001B[39m         \u001B[43mfuture\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2822\u001B[39m     result = worker_fn()\n\u001B[32m   2823\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:59\u001B[39m, in \u001B[36m_WorkItem.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     56\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     61\u001B[39m     \u001B[38;5;28mself\u001B[39m.future.set_exception(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2851\u001B[39m, in \u001B[36m_worker_compile_cpp\u001B[39m\u001B[34m(lock_path, cpp_builders)\u001B[39m\n\u001B[32m   2849\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m builder \u001B[38;5;129;01min\u001B[39;00m cpp_builders:\n\u001B[32m   2850\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(builder.get_target_file_path()):\n\u001B[32m-> \u001B[39m\u001B[32m2851\u001B[39m         \u001B[43mbuilder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:2020\u001B[39m, in \u001B[36mCppBuilder.build\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2017\u001B[39m _create_if_dir_not_exist(_build_tmp_dir)\n\u001B[32m   2019\u001B[39m build_cmd = \u001B[38;5;28mself\u001B[39m.get_command_line()\n\u001B[32m-> \u001B[39m\u001B[32m2020\u001B[39m \u001B[43mrun_compile_cmd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuild_cmd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_build_tmp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2021\u001B[39m _remove_dir(_build_tmp_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:593\u001B[39m, in \u001B[36mrun_compile_cmd\u001B[39m\u001B[34m(cmd_line, cwd)\u001B[39m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_compile_cmd\u001B[39m(cmd_line: \u001B[38;5;28mstr\u001B[39m, cwd: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    592\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m dynamo_timed(\u001B[33m\"\u001B[39m\u001B[33mcompile_file\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m593\u001B[39m         \u001B[43m_run_compile_cmd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd_line\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:588\u001B[39m, in \u001B[36m_run_compile_cmd\u001B[39m\u001B[34m(cmd_line, cwd)\u001B[39m\n\u001B[32m    578\u001B[39m     instruction = (\n\u001B[32m    579\u001B[39m         \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mOpenMP support not found. Please try one of the following solutions:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    580\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    585\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m with `include/omp.h` under it.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    586\u001B[39m     )\n\u001B[32m    587\u001B[39m     output += instruction\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc.CppCompileError(cmd, output) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mInductorError\u001B[39m: CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/q5/cq5plabjg5pvzrcrbtktyiynoyjujrplw7pbtmfiecohqrunp5zx.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -include /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/precompiled_headers/c7mv6jl773tdyux4rvslwqz7eud43eht3ks5rt5kvi2w5u5s4rr4.h -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include -I/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -o /var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/torchinductor_ankushraj/q5/cq5plabjg5pvzrcrbtktyiynoyjujrplw7pbtmfiecohqrunp5zx.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib\n\nOutput:\nclang++: error: no such file or directory: 'Sun'\nclang++: error: no such file or directory: 'Labs'\nclang++: error: no such file or directory: 'Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/.venv/lib/python3.13/site-packages/torch/lib'\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) Optional: quick timing, profiler, memory",
   "id": "65c9b4155bf0b0cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:45:24.650889Z",
     "start_time": "2025-10-18T18:45:22.351657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "t0 = time.perf_counter()\n",
    "xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "    _, loss = model(xb, yb)\n",
    "loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "print('One-step seconds:', time.perf_counter() - t0)"
   ],
   "id": "cee0fa4e25a5487b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/4162582575.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-step seconds: 2.2943923339998946\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:45:51.142717Z",
     "start_time": "2025-10-18T18:45:36.068303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    from torch.profiler import profile, ProfilerActivity\n",
    "    acts = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available(): acts.append(ProfilerActivity.CUDA)\n",
    "    with profile(activities=acts, record_shapes=True, profile_memory=True) as prof:\n",
    "        for _ in range(3):\n",
    "            xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "            dtype = torch.bfloat16 if (USE_AMP and torch.cuda.is_available()) else torch.float32\n",
    "            with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "                _, loss = model(xb, yb)\n",
    "            loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "    print(prof.key_averages().table(sort_by='self_cuda_time_total' if torch.cuda.is_available() else 'cpu_time_total', row_limit=20))\n",
    "except Exception as e:\n",
    "    print('Profiler not available or failed:', repr(e))"
   ],
   "id": "fc15d179216e8a1e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1019 00:15:36.508983000 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "/var/folders/lf/6_tqrqqs0w5_3q_cz1dtnmtr0000gn/T/ipykernel_1990/3122675649.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: CheckpointFunct...         0.17%      11.534ms        49.80%        3.283s     218.852ms     -78.55 MB    -360.00 MB            15  \n",
      "                             CheckpointFunctionBackward         0.62%      40.800ms        49.62%        3.271s     218.083ms     281.45 MB      -3.69 GB            15  \n",
      "                                          aten::dropout         0.12%       7.706ms        28.14%        1.855s      18.188ms       3.66 GB      -1.05 GB           102  \n",
      "                                     CheckpointFunction         1.12%      73.751ms        27.55%        1.816s     121.086ms     180.07 MB      -7.03 GB            15  \n",
      "                                       aten::bernoulli_        25.36%        1.672s        25.36%        1.672s      16.390ms           0 B           0 B           102  \n",
      "                                               aten::mm        23.90%        1.576s        23.90%        1.576s       7.195ms       3.94 GB       3.94 GB           219  \n",
      "                                           aten::linear         0.01%     760.119us        17.09%        1.127s       8.347ms       4.21 GB           0 B           135  \n",
      "                                           aten::matmul         0.09%       5.726ms        13.14%     866.063ms       6.415ms       4.85 GB    -540.00 MB           135  \n",
      "                                            aten::addmm         9.06%     597.474ms         9.91%     653.364ms       9.899ms       1.93 GB       1.93 GB            66  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.31%      20.603ms         9.62%     633.948ms      17.610ms    -998.87 MB      -2.11 GB            36  \n",
      "                                              aten::bmm         8.39%     553.080ms         9.56%     630.509ms       4.569ms       3.41 GB       3.41 GB           138  \n",
      "                                         AddmmBackward0         0.01%     494.996us         8.72%     574.666ms      15.963ms       1.13 GB           0 B            36  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.33%      22.010ms         8.42%     554.729ms      14.224ms      -1.50 GB      -2.03 GB            39  \n",
      "                                            MmBackward0         0.01%     419.129us         8.08%     532.720ms      13.659ms     543.66 MB           0 B            39  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.36%      23.609ms         5.03%     331.815ms       9.217ms      -1.05 GB      -2.53 GB            36  \n",
      "                                           BmmBackward0         0.00%     311.867us         4.68%     308.206ms       8.561ms       1.48 GB           0 B            36  \n",
      "                                            aten::copy_         3.78%     249.014ms         3.78%     249.014ms     509.231us           0 B           0 B           489  \n",
      "                                             aten::gelu         3.75%     247.095ms         3.75%     247.095ms       7.488ms       1.55 GB       1.55 GB            33  \n",
      "     autograd::engine::evaluate_function: GeluBackward0         0.28%      18.225ms         2.96%     195.054ms      10.836ms    -864.00 MB      -1.69 GB            18  \n",
      "                                            aten::clone         0.01%     895.040us         2.95%     194.507ms     820.703us       4.57 GB           0 B           237  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.592s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:46:05.456547Z",
     "start_time": "2025-10-18T18:46:05.450013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    xb, yb = get_batch(train_stream, BLOCK_SIZE, BATCH_SIZE, device)\n",
    "    dtype = torch.bfloat16 if USE_AMP else torch.float32\n",
    "    with autocast(enabled=USE_AMP, dtype=dtype if USE_AMP else None):\n",
    "        _, loss = model(xb, yb)\n",
    "    loss.backward(); opt.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    print(f'Peak CUDA memory this cell: {peak_mb:.1f} MB')"
   ],
   "id": "998b295561b77990",
   "outputs": [],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
