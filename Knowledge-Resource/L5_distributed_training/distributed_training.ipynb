{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🚀 Distributed Training in PyTorch: **DDP & FSDP** — Complete, Working Notebook\n",
    "\n",
    "This notebook gives you **end-to-end, working code** for both **DDP (DistributedDataParallel)** and **FSDP (FullyShardedDataParallel)** using a tiny GPT-style model and a synthetic token-stream dataset.\n",
    "\n",
    "It is designed to **actually run** across 1+ GPUs (or CPU) by writing runnable training scripts and launching them via `torchrun`.\n",
    "\n",
    "## What you get\n",
    "- Minimal yet realistic **MiniGPT** model (decoder-only)\n",
    "- **Synthetic token-stream** dataset (no external data required)\n",
    "- **AMP**, **Grad Accum**, **Cosine LR warmup/decay**, gradient **clipping**\n",
    "- **DDP trainer** and **FSDP trainer** scripts generated to disk\n",
    "- **Checkpoint save/load** (correct unwrapping), rank-0 logging\n",
    "- **GPU/CPU** compatible (NCCL on CUDA, GLOO otherwise) — no `torch.compile`\n",
    "- Lots of comments + troubleshooting tips\n",
    "\n",
    "### How to use\n",
    "1. Run each code cell **in order**.\n",
    "2. Use the provided **launcher cells** to run DDP or FSDP with `torchrun`.\n",
    "3. If you're on CPU only, you can still run with `--nproc_per_node=1` to validate end-to-end behavior.\n",
    "\n",
    "> ⚠️ Multi-process distributed training is best launched from the shell. This notebook writes training scripts that you can run from here or from your terminal."
   ],
   "id": "a510c08480428b60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T19:09:09.324706Z",
     "start_time": "2025-10-18T19:09:08.430136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0) Environment sanity + global guards (disable PT2 compile just in case)\n",
    "!pip -q install torch tokenizers tqdm\n",
    "import os, torch\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "try:\n",
    "    torch._dynamo.reset()\n",
    "except Exception:\n",
    "    pass\n",
    "print('PyTorch:', torch.__version__, '| CUDA available:', torch.cuda.is_available(), '| GPUs:', torch.cuda.device_count())"
   ],
   "id": "23d053f07b92377e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "PyTorch: 2.9.0 | CUDA available: False | GPUs: 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1) Shared utilities: model + dataset (written to `lib_minigpt.py`): create a tiny GPT-style model and a simple token-stream dataset. Both trainers import this file",
   "id": "ab6686e2a8fd5ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T19:20:31.544938Z",
     "start_time": "2025-10-18T19:20:31.533675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run this once to (re)generate the 3 files and verify they exist.\n",
    "from pathlib import Path\n",
    "\n",
    "lib_code = r\"\"\"import math, torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.qkv = nn.Linear(n_embed, 3 * n_embed, bias=False)\n",
    "        self.proj = nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer('mask', mask.view(1,1,block_size,block_size))\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        qkv = self.qkv(x); q,k,v = qkv.chunk(3, dim=-1)\n",
    "        nh = self.n_head\n",
    "        q = q.view(B,T,nh,-1).transpose(1,2)\n",
    "        k = k.view(B,T,nh,-1).transpose(1,2)\n",
    "        v = v.view(B,T,nh,-1).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
    "        att = self.attn_drop(att.softmax(dim=-1))\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,-1)\n",
    "        return self.resid_drop(self.proj(y))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = CausalSelfAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = nn.Sequential(nn.Linear(n_embed, 4*n_embed), nn.GELU(),\n",
    "                                 nn.Linear(4*n_embed, n_embed), nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=384, n_head=6, n_layer=6, block_size=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.drop(self.tok_emb(idx) + self.pos_emb(pos)[None,:,:])\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        logits = self.head(self.ln_f(x))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "def make_stream(n_tokens:int, vocab_size:int):\n",
    "    return torch.randint(0, vocab_size, (n_tokens,), dtype=torch.long)\n",
    "\n",
    "class TokenStreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor_data, block_size):\n",
    "        self.data = tensor_data\n",
    "        self.block = block_size\n",
    "    def __len__(self): return max(0, len(self.data) - self.block - 1)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.data[i:i+self.block]\n",
    "        y = self.data[i+1:i+1+self.block]\n",
    "        return x, y\n",
    "\"\"\"\n",
    "\n",
    "ddp_code = r\"\"\"import os, math, torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from lib_minigpt import MiniGPT, make_stream, TokenStreamDataset\n",
    "\n",
    "def setup_dist():\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    dist.init_process_group(backend=backend)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "def is_main():\n",
    "    return int(os.environ.get('RANK', '0')) == 0\n",
    "\n",
    "def cosine_factor(step, max_steps, warmup=200, min_factor=0.1):\n",
    "    if step < warmup:\n",
    "        return max(1e-8, (step+1)/max(1, warmup))\n",
    "    progress = (step - warmup) / max(1, max_steps - warmup)\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi*progress))\n",
    "\n",
    "def main():\n",
    "    setup_dist()\n",
    "    device = torch.device(f\"cuda:{os.environ.get('LOCAL_RANK','0')}\" if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        try: torch.set_float32_matmul_precision('medium')\n",
    "        except Exception: pass\n",
    "\n",
    "    BLOCK = int(os.environ.get('BLOCK_SIZE', '256'))\n",
    "    VOCAB = int(os.environ.get('VOCAB_SIZE', '8000'))\n",
    "    TRAIN_TOK = int(os.environ.get('TRAIN_TOKENS', '600000'))\n",
    "    VAL_TOK   = int(os.environ.get('VAL_TOKENS',   '60000'))\n",
    "    BATCH = int(os.environ.get('BATCH_SIZE', '32'))\n",
    "    ACCUM = int(os.environ.get('GRAD_ACCUM', '2'))\n",
    "    LR    = float(os.environ.get('LR', '3e-4'))\n",
    "    MAX_STEPS = int(os.environ.get('MAX_STEPS', '300'))\n",
    "    WARMUP = int(os.environ.get('WARMUP_STEPS', '100'))\n",
    "    CLIP = float(os.environ.get('CLIP_NORM', '1.0'))\n",
    "    USE_AMP = torch.cuda.is_available()\n",
    "\n",
    "    train_stream = make_stream(TRAIN_TOK, VOCAB)\n",
    "    val_stream   = make_stream(VAL_TOK,   VOCAB)\n",
    "    train_ds = TokenStreamDataset(train_stream, BLOCK)\n",
    "    val_ds   = TokenStreamDataset(val_stream,   BLOCK)\n",
    "    train_samp = DistributedSampler(train_ds, shuffle=True, drop_last=True)\n",
    "    val_samp   = DistributedSampler(val_ds,   shuffle=False, drop_last=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, sampler=train_samp, num_workers=4, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH, sampler=val_samp,   num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = MiniGPT(VOCAB, n_embed=384, n_head=6, n_layer=6, block_size=BLOCK, dropout=0.1).to(device)\n",
    "    model = DDP(model, device_ids=[device] if device.type == 'cuda' else None)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    def run_val():\n",
    "        model.eval()\n",
    "        total, count = torch.tensor(0.0, device=device), torch.tensor(0, device=device)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                _, loss = model(xb, yb)\n",
    "                total += loss.detach(); count += 1\n",
    "        dist.all_reduce(total, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(count, op=dist.ReduceOp.SUM)\n",
    "        model.train()\n",
    "        return (total / count).item() if int(count.item())>0 else float('nan')\n",
    "\n",
    "    # Tiny warmup\n",
    "    for _ in range(2):\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb, yb)\n",
    "            loss.backward(); opt.zero_grad(set_to_none=True); break\n",
    "\n",
    "    step = 0\n",
    "    while step < MAX_STEPS:\n",
    "        train_samp.set_epoch(step)\n",
    "        for xb, yb in train_dl:\n",
    "            fac = cosine_factor(step, MAX_STEPS, warmup=WARMUP, min_factor=0.1)\n",
    "            for pg in opt.param_groups: pg['lr'] = LR * fac\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb.to(device, non_blocking=True), yb.to(device, non_blocking=True))\n",
    "                loss = loss / max(1, ACCUM)\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step+1) % ACCUM == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            step += 1\n",
    "            if step % 100 == 0 or step == MAX_STEPS:\n",
    "                if is_main():\n",
    "                    vl = run_val()\n",
    "                    print(f\"step {step}/{MAX_STEPS} | val_loss {vl:.4f}\")\n",
    "            if step >= MAX_STEPS: break\n",
    "\n",
    "    if is_main():\n",
    "        torch.save(model.module.state_dict(), 'minigpt_ddp.pt')\n",
    "    dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "fsdp_code = r\"\"\"import os, math, torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import checkpoint_wrapper\n",
    "from lib_minigpt import MiniGPT, make_stream, TokenStreamDataset, Block\n",
    "\n",
    "def setup_dist():\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    dist.init_process_group(backend=backend)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "def is_main():\n",
    "    return int(os.environ.get('RANK', '0')) == 0\n",
    "\n",
    "def cosine_factor(step, max_steps, warmup=200, min_factor=0.1):\n",
    "    if step < warmup:\n",
    "        return max(1e-8, (step+1)/max(1, warmup))\n",
    "    progress = (step - warmup) / max(1, max_steps - warmup)\n",
    "    return min_factor + 0.5*(1-min_factor)*(1 + math.cos(math.pi*progress))\n",
    "\n",
    "def main():\n",
    "    setup_dist()\n",
    "    device = torch.device(f\"cuda:{os.environ.get('LOCAL_RANK','0')}\" if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        try: torch.set_float32_matmul_precision('medium')\n",
    "        except Exception: pass\n",
    "\n",
    "    BLOCK = int(os.environ.get('BLOCK_SIZE', '256'))\n",
    "    VOCAB = int(os.environ.get('VOCAB_SIZE', '8000'))\n",
    "    TRAIN_TOK = int(os.environ.get('TRAIN_TOKENS', '600000'))\n",
    "    VAL_TOK   = int(os.environ.get('VAL_TOKENS',   '60000'))\n",
    "    BATCH = int(os.environ.get('BATCH_SIZE', '32'))\n",
    "    LR    = float(os.environ.get('LR', '3e-4'))\n",
    "    MAX_STEPS = int(os.environ.get('MAX_STEPS', '300'))\n",
    "    WARMUP = int(os.environ.get('WARMUP_STEPS', '100'))\n",
    "    USE_AMP = torch.cuda.is_available()\n",
    "\n",
    "    train_stream = make_stream(TRAIN_TOK, VOCAB)\n",
    "    val_stream   = make_stream(VAL_TOK,   VOCAB)\n",
    "    train_ds = TokenStreamDataset(train_stream, BLOCK)\n",
    "    val_ds   = TokenStreamDataset(val_stream,   BLOCK)\n",
    "    train_samp = DistributedSampler(train_ds, shuffle=True, drop_last=True)\n",
    "    val_samp   = DistributedSampler(val_ds,   shuffle=False, drop_last=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, sampler=train_samp, num_workers=4, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH, sampler=val_samp,   num_workers=2, pin_memory=True)\n",
    "\n",
    "    base_model = MiniGPT(VOCAB, n_embed=384, n_head=6, n_layer=6, block_size=BLOCK, dropout=0.1)\n",
    "\n",
    "    # optional activation checkpointing per block\n",
    "    for i, blk in enumerate(base_model.blocks):\n",
    "        base_model.blocks[i] = checkpoint_wrapper(blk)\n",
    "\n",
    "    auto_wrap = transformer_auto_wrap_policy({Block})\n",
    "\n",
    "    mp_policy = None\n",
    "    if torch.cuda.is_available():\n",
    "        from torch.distributed.fsdp import MixedPrecision\n",
    "        mp_policy = MixedPrecision(param_dtype=torch.bfloat16, reduce_dtype=torch.bfloat16, buffer_dtype=torch.bfloat16)\n",
    "\n",
    "    model = FSDP(base_model, auto_wrap_policy=auto_wrap, mixed_precision=mp_policy).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_val():\n",
    "        model.eval()\n",
    "        total, count = torch.tensor(0.0, device=device), torch.tensor(0, device=device)\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb, yb)\n",
    "            total += loss.detach(); count += 1\n",
    "        dist.all_reduce(total, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(count, op=dist.ReduceOp.SUM)\n",
    "        model.train()\n",
    "        return (total / count).item() if int(count.item())>0 else float('nan')\n",
    "\n",
    "    # tiny warmup\n",
    "    for _ in range(2):\n",
    "        for xb, yb in train_dl:\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb.to(device), yb.to(device))\n",
    "            loss.backward(); opt.zero_grad(set_to_none=True); break\n",
    "\n",
    "    step = 0\n",
    "    ACCUM = 2\n",
    "    CLIP = 1.0\n",
    "    while step < MAX_STEPS:\n",
    "        train_samp.set_epoch(step)\n",
    "        for xb, yb in train_dl:\n",
    "            fac = cosine_factor(step, MAX_STEPS, warmup=WARMUP, min_factor=0.1)\n",
    "            for pg in opt.param_groups: pg['lr'] = float(LR) * fac\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb.to(device, non_blocking=True), yb.to(device, non_blocking=True))\n",
    "                loss = loss / ACCUM\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step+1) % ACCUM == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            step += 1\n",
    "            if step % 100 == 0 or step == MAX_STEPS:\n",
    "                if is_main():\n",
    "                    vl = run_val()\n",
    "                    print(f\"step {step}/{MAX_STEPS} | val_loss {vl:.4f}\")\n",
    "            if step >= MAX_STEPS: break\n",
    "\n",
    "    if is_main():\n",
    "        from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n",
    "        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT,\n",
    "                                  FullStateDictConfig(offload_to_cpu=True, rank0_only=True)):\n",
    "            sd = model.state_dict()\n",
    "        torch.save(sd, 'minigpt_fsdp_full.pt')\n",
    "\n",
    "    dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# write files\n",
    "Path('lib_minigpt.py').write_text(lib_code)\n",
    "Path('train_ddp_minigpt.py').write_text(ddp_code)\n",
    "Path('train_fsdp_minigpt.py').write_text(fsdp_code)\n",
    "\n",
    "# verify\n",
    "missing = [p for p in ['lib_minigpt.py','train_ddp_minigpt.py','train_fsdp_minigpt.py'] if not Path(p).exists()]\n",
    "print(\"Wrote files.\" if not missing else f\"Missing: {missing}\")\n"
   ],
   "id": "947636595af2e110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote files.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3) Write FSDP trainer (`train_fsdp_minigpt.py`)\n",
    "    - this script uses PyTorch FSDP for sharding params/grad/optim state across GPUs\n",
    "        - Mixed precision (`bf16` if CUDA available)\n",
    "        - Auto-wrap transformer blocks\n",
    "        - Full-state-dict checkpointing (rank 0)\n",
    "        NOTE: FSDP on CPU will run, but its benefit is when you have multiple GPUs.\n"
   ],
   "id": "b229bfc2e8dba959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T19:20:44.632916Z",
     "start_time": "2025-10-18T19:20:44.626302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fsdp_path = Path('train_fsdp_minigpt.py')\n",
    "fsdp_code = r'''import os, math, time, torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import StateDictType, FullStateDictConfig\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import checkpoint_wrapper\n",
    "from lib_minigpt import MiniGPT, make_stream, TokenStreamDataset, Block\n",
    "\n",
    "def setup_dist():\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    dist.init_process_group(backend=backend)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "def is_main():\n",
    "    return int(os.environ.get('RANK', '0')) == 0\n",
    "\n",
    "def cosine_factor(step, max_steps, warmup=200, min_factor=0.1):\n",
    "    if step < warmup:\n",
    "        return max(1e-8, (step+1)/max(1, warmup))\n",
    "    progress = (step - warmup) / max(1, max_steps - warmup)\n",
    "    return min_factor + 0.5 * (1-min_factor) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "def main():\n",
    "    setup_dist()\n",
    "    device = torch.device(f\"cuda:{os.environ['LOCAL_RANK']}\" if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        try: torch.set_float32_matmul_precision('medium')\n",
    "        except Exception: pass\n",
    "\n",
    "    # Config\n",
    "    BLOCK = int(os.environ.get('BLOCK_SIZE', '256'))\n",
    "    VOCAB = int(os.environ.get('VOCAB_SIZE', '8000'))\n",
    "    TRAIN_TOK = int(os.environ.get('TRAIN_TOKENS', '600000'))\n",
    "    VAL_TOK   = int(os.environ.get('VAL_TOKENS',   '60000'))\n",
    "    BATCH = int(os.environ.get('BATCH_SIZE', '32'))\n",
    "    ACCUM = int(os.environ.get('GRAD_ACCUM', '2'))\n",
    "    LR    = float(os.environ.get('LR', '3e-4'))\n",
    "    MAX_STEPS = int(os.environ.get('MAX_STEPS', '500'))\n",
    "    WARMUP = int(os.environ.get('WARMUP_STEPS', '200'))\n",
    "    CLIP = float(os.environ.get('CLIP_NORM', '1.0'))\n",
    "    USE_AMP = torch.cuda.is_available()\n",
    "\n",
    "    # Data\n",
    "    train_stream = make_stream(TRAIN_TOK, VOCAB)\n",
    "    val_stream   = make_stream(VAL_TOK,   VOCAB)\n",
    "    train_ds = TokenStreamDataset(train_stream, BLOCK)\n",
    "    val_ds   = TokenStreamDataset(val_stream,   BLOCK)\n",
    "    train_samp = DistributedSampler(train_ds, shuffle=True, drop_last=True)\n",
    "    val_samp   = DistributedSampler(val_ds,   shuffle=False, drop_last=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, sampler=train_samp, num_workers=4, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH, sampler=val_samp,   num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Base model on CPU first (FSDP will move shards to GPU)\n",
    "    base_model = MiniGPT(VOCAB, n_embed=384, n_head=6, n_layer=6, block_size=BLOCK, dropout=0.1)\n",
    "\n",
    "    # Optional activation checkpointing on each transformer Block for memory\n",
    "    for i, blk in enumerate(base_model.blocks):\n",
    "        base_model.blocks[i] = checkpoint_wrapper(blk)\n",
    "\n",
    "    # Auto-wrap policy for transformer Blocks\n",
    "    auto_wrap = transformer_auto_wrap_policy({Block})\n",
    "\n",
    "    # Mixed precision config for FSDP (use bf16 on CUDA if available)\n",
    "    mp_policy = None\n",
    "    if torch.cuda.is_available():\n",
    "        from torch.distributed.fsdp import MixedPrecision\n",
    "        mp_policy = MixedPrecision(param_dtype=torch.bfloat16, reduce_dtype=torch.bfloat16, buffer_dtype=torch.bfloat16)\n",
    "\n",
    "    # Wrap with FSDP\n",
    "    model = FSDP(base_model, auto_wrap_policy=auto_wrap, mixed_precision=mp_policy)\n",
    "    model = model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    def run_val():\n",
    "        model.eval()\n",
    "        total, count = torch.tensor(0.0, device=device), torch.tensor(0, device=device)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                    _, loss = model(xb, yb)\n",
    "                total += loss.detach()\n",
    "                count += 1\n",
    "        dist.all_reduce(total, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(count, op=dist.ReduceOp.SUM)\n",
    "        model.train()\n",
    "        return (total / count).item() if int(count.item())>0 else float('nan')\n",
    "\n",
    "    # Warmup micro-steps\n",
    "    for _ in range(3):\n",
    "        for xb, yb in train_dl:\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb.to(device), yb.to(device))\n",
    "            loss.backward(); opt.zero_grad(set_to_none=True); break\n",
    "\n",
    "    # Train\n",
    "    step = 0\n",
    "    while step < MAX_STEPS:\n",
    "        train_samp.set_epoch(step)\n",
    "        for xb, yb in train_dl:\n",
    "            fac = cosine_factor(step, MAX_STEPS, warmup=WARMUP, min_factor=0.1)\n",
    "            for pg in opt.param_groups: pg['lr'] = LR * fac\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_AMP else None):\n",
    "                _, loss = model(xb.to(device, non_blocking=True), yb.to(device, non_blocking=True))\n",
    "                loss = loss / 2  # ACCUM fixed to 2 for clarity\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step+1) % 2 == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            step += 1\n",
    "            if step % 100 == 0 or step == MAX_STEPS:\n",
    "                if is_main():\n",
    "                    vl = run_val()\n",
    "                    print(f\"step {step}/{MAX_STEPS} | val_loss {vl:.4f}\")\n",
    "            if step >= MAX_STEPS: break\n",
    "\n",
    "    # Save full state dict on rank 0\n",
    "    if is_main():\n",
    "        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, FullStateDictConfig(offload_to_cpu=True, rank0_only=True)):\n",
    "            sd = model.state_dict()\n",
    "        torch.save(sd, 'minigpt_fsdp_full.pt')\n",
    "\n",
    "    dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "fsdp_path.write_text(fsdp_code)\n",
    "print(f'Wrote {fsdp_path.resolve()}')"
   ],
   "id": "f6d7c87d0b21dd74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/ankushraj/Desktop/Rising Sun Labs Resource/R-S-L-Repositories/SunForgeLLM/SunForgeLLM/Knowledge-Resource/L5_distributed_training/train_fsdp_minigpt.py\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Launchers — How to run DDP / FSDP from here\n",
    "\n",
    "### DDP (single machine)\n",
    "Use all visible GPUs:\n",
    "```bash\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=$(python - <<'PY'\\nimport torch; print(torch.cuda.device_count() or 1)\\nPY) \\\n",
    "  train_ddp_minigpt.py\n",
    "```\n",
    "Or pick specific GPUs:\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nnodes=1 --nproc_per_node=2 train_ddp_minigpt.py\n",
    "```\n",
    "\n",
    "### FSDP (single machine)\n",
    "```bash\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=$(python - <<'PY'\\nimport torch; print(torch.cuda.device_count() or 1)\\nPY) \\\n",
    "  train_fsdp_minigpt.py\n",
    "```\n",
    "\n",
    "> **CPU-only test:** set `--nproc_per_node=1`; backend switches to GLOO automatically.\n",
    "\n",
    "You can also tweak env variables inline, for example:\n",
    "```bash\n",
    "BLOCK_SIZE=256 VOCAB_SIZE=8000 MAX_STEPS=200 BATCH_SIZE=32 \\\n",
    "torchrun --standalone --nproc_per_node=2 train_ddp_minigpt.py\n",
    "```"
   ],
   "id": "da3020ece6d71ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) (Optional) Run a tiny smoke test locally\n",
    "This just verifies imports and that scripts exist. For true distributed runs, use the launcher commands above."
   ],
   "id": "4fb9e37538679193"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T19:20:56.904893Z",
     "start_time": "2025-10-18T19:20:56.647838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib.util, sys\n",
    "for p in ['lib_minigpt.py', 'train_ddp_minigpt.py', 'train_fsdp_minigpt.py']:\n",
    "    assert Path(p).exists(), f'Missing {p}'\n",
    "print('Files present ✅')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('lib_minigpt', 'lib_minigpt.py')\n",
    "m = importlib.util.module_from_spec(spec); spec.loader.exec_module(m)\n",
    "model = m.MiniGPT(vocab_size=8000, block_size=256)\n",
    "xb = torch.randint(0, 8000, (2,256)); yb = torch.randint(0,8000,(2,256))\n",
    "with torch.no_grad():\n",
    "    _, loss = model(xb, yb)\n",
    "print('Model forward OK, loss=', float(loss))"
   ],
   "id": "eba21803b57b94f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files present ✅\n",
      "Model forward OK, loss= 9.060871124267578\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
